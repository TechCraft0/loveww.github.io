[{"title":"Hello World","url":"/2026/02/01/01-%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%80%83/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["个人思考"]},{"title":"我的第一篇 Butterfly 博客","url":"/2025/09/20/01-%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%80%83/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87-Butterfly-%E5%8D%9A%E5%AE%A2/","content":"1. 环境准备在开始之前，需要先准备好环境：\n\n安装 Node.js（推荐 LTS 版本）\n安装 Git\n\n安装完成后，检查版本：\nnode -vnpm -vgit --version````## 2. 安装 Hexo在你想存放博客的文件夹中执行：```bashnpm install -g hexo-clihexo init myblogcd myblognpm install\n\n然后启动本地服务：\nhexo s\n\n浏览器访问 http://localhost:4000，就能看到默认页面。\n3. 更换 Butterfly 主题进入博客目录 myblog，下载 Butterfly 主题：\ngit clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly\n\n修改 _config.yml 文件，把主题改成：\ntheme: butterfly\n\n安装必要依赖：\nnpm install hexo-renderer-pug hexo-renderer-stylus --save\n\n重启服务后，你就能看到漂亮的 Butterfly 界面啦。\n4. 写文章使用命令新建文章：\nhexo new post &quot;我的第一篇文章&quot;\n\n文章会生成在 source/_posts/ 目录下，你可以用 Markdown 来写内容。\n5. 部署到 GitHub Pages\n在 GitHub 创建一个仓库，命名为：你的用户名.github.io\n安装部署插件：\n\nnpm install hexo-deployer-git --save\n\n\n编辑 _config.yml，加入部署配置：\n\ndeploy:  type: git  repo: https://github.com/你的用户名/你的用户名.github.io.git  branch: main\n\n\n执行命令部署：\n\nhexo cleanhexo ghexo d\n\n部署完成后，访问 https://你的用户名.github.io 就能看到你的博客啦！\n6. 常用命令\nhexo new post &quot;文章标题&quot; ：新建文章\nhexo s ：启动本地服务器\nhexo g ：生成静态文件\nhexo d ：部署到远程\n\n总结从零开始搭建博客其实并不难，主要流程就是：\n👉 安装 Hexo → 使用 Butterfly 主题 → 写文章 → 部署上线\n之后你还可以：\n\n美化主题（添加背景、切换字体、增加特效）\n配置评论系统（Waline、Gitalk、Twikoo 等）\n添加站点统计（不蒜子、百度统计、Google Analytics）\n\n未来再继续折腾！🚀\n\n\n","categories":["个人思考"],"tags":["Hexo","Butterfly","初学"]},{"title":"Lidar 地面分割算法对比及实现","url":"/2025/09/22/02-%E4%BC%A0%E6%84%9F%E5%99%A8%E6%8A%80%E6%9C%AF/Lidar-%E5%9C%B0%E9%9D%A2%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94%E5%8F%8A%E5%AE%9E%E7%8E%B0/","content":"","categories":["传感器技术"]},{"title":"Lidar 地面分割算法综述","url":"/2025/09/22/02-%E4%BC%A0%E6%84%9F%E5%99%A8%E6%8A%80%E6%9C%AF/Lidar-%E5%9C%B0%E9%9D%A2%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/","content":"","categories":["传感器技术"]},{"title":"Lidar重复式扫描和非重复式扫描区别及主要特性","url":"/2025/09/22/02-%E4%BC%A0%E6%84%9F%E5%99%A8%E6%8A%80%E6%9C%AF/Lidar%E9%87%8D%E5%A4%8D%E5%BC%8F%E6%89%AB%E6%8F%8F%E5%92%8C%E9%9D%9E%E9%87%8D%E5%A4%8D%E5%BC%8F%E6%89%AB%E6%8F%8F%E5%8C%BA%E5%88%AB%E5%8F%8A%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7/","content":"自21世纪初激光雷达在DARPA无人车大赛上首次亮相以来，在过去十余年的自动驾驶行业发展浪潮中，被主流认可的激光雷达形态长期属于机械式激光雷达。非重复是扫描是livox &#x3D;&#x3D;览沃&#x3D;&#x3D;在2020年初的CES展会上，Livox面向混合固态时代打造的浩界系列首款激光雷达Horizon横空出世，其7000元级的售价除了给市场提供一剂“激光雷达用得起、买得到”的强心针以外，也给自动驾驶感知算法领域带来了一个新事物：基于旋转棱镜扫描模式下的非重复点云形态。那么下文将会对以重复式扫描的代表机械式激光雷达和非重复式扫描的代表的Livox Mid360激光雷达进行对比分析。\n1. 什么是重复式扫描和非重复式扫描？&gt; \n\n参考- https://zhuanlan.zhihu.com/p/551589651\n\n","categories":["传感器技术"],"tags":["Lidar","重复式扫描","非重复式扫描"]},{"title":"YOLO系列-《YOLOv1：将检测视为回归的极简美学》","url":"/2026/01/03/03-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/YOLO%E7%B3%BB%E5%88%97-YOLOv1%EF%BC%9A%E5%B0%86%E6%A3%80%E6%B5%8B%E8%A7%86%E4%B8%BA%E5%9B%9E%E5%BD%92%E7%9A%84%E6%9E%81%E7%AE%80%E7%BE%8E%E5%AD%A6/","content":"前言","categories":["目标检测"],"tags":["YOLOv1","数据增强","YOLO系列"]},{"title":"寻根溯源：我的 YOLO 全系列深度解析之旅 (开篇)","url":"/2026/02/01/03-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%AF%BB%E6%A0%B9%E6%BA%AF%E6%BA%90%EF%BC%9A%E6%88%91%E7%9A%84-YOLO-%E5%85%A8%E7%B3%BB%E5%88%97%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E4%B9%8B%E6%97%85-%E5%BC%80%E7%AF%87/","content":"寻根溯源：我的 YOLO 全系列深度解析之旅 (开篇)0. 前言：为什么要写这个系列？在计算机视觉（CV）的江湖里，YOLO (You Only Look Once) 是一个绕不开的名字。从 2015 年 Redmon 提出 v1 版本至今，YOLO 系列已经走过了近十个年头，演进到了如今的 v11。\n作为一名算法从业者，我几乎每天都在和各种版本的 YOLO 打交道：调参、部署、优化。但在高强度的工作节奏下，我发现自己陷入了一个**“熟练的陌生人”**误区：\n\n对抗遗忘： 算法的演进非常细碎。从 Anchor-based 到 Anchor-free，从简单的 IoU 到复杂的 $CIoU$、$DIoU$ 损失函数，如果不系统整理，很多细节（如正负样本匹配策略的改变）很容易在脑海中变得模糊。\n构建底层逻辑： 仅仅跑通 train.py 是远远不够的。为了在面试中对答如流，也为了在实际业务中能根据场景选择最合适的版本，我需要从底层原理出发，理解每一代 YOLO “为什么要这么改”。\n职业背书： 博客是最好的技术名片。通过深度解析，我希望向同行和面试官展示：我不仅能通过工具解决问题，更能洞察算法背后的设计哲学。\n\n\n1. 核心目标：整理、理解、内化这个系列不仅仅是论文的翻译，更是我个人的复习笔记和实战心得。我将重点攻克以下“硬骨头”：\n🔍 深度拆解每一代演进我会详细说明每个版本的改进原理，包括但不限于：\n\n网络架构 (Backbone, Neck, Head)： 每一代在特征提取和融合上做了哪些“加减法”？\n损失函数 (Loss Function)： 为什么分类损失和回归损失在不断迭代？\n匹配策略 (Label Assignment)： 详解从 Max IoU 到 SimOTA，再到 Task-Aligned Assigner 的逻辑跃迁。\n工程 Trick： 数据增强 (Mosaic)、重参数化 (RepVGG)、解耦头等技术的实际意义。\n\n📊 建立纵向知识地图我会复现并横向对比不同版本的性能指标，分析它们在 精度 ($mAP$) 与 速度 ($FPS$) 之间的权衡方案。\n\n2. 学习路线图 (Roadmap)为了方便复习，我将按以下节奏进行更新：\n\n\n\n阶段\n涵盖版本\n核心复习点\n\n\n\n第一阶段：奠基\nYOLOv1 - v3\n单阶段检测框架、Grid Cell 概念、Anchor 与 Darknet 架构\n\n\n第二阶段：巅峰工程\nYOLOv4 - v5\n数据增强 (Mosaic)、SPP&#x2F;PAN 结构、工程化的极致优化\n\n\n第三阶段：现代变革\nYOLOv6 - v8\nAnchor-free 时代、解耦头 (Decoupled Head)、动态匹配策略\n\n\n第四阶段：前沿探索\nYOLOv9 - v11\n可编程梯度信息 (PGI)、无 NMS 设计、效率与精度的极限\n\n\n\n3. 写在最后\n“Paper is cheap, show me the logic.”\n\n这个系列是我个人对目标检测领域的一次深度致敬。如果你也和我一样，经常在各个 YOLO 版本间“反复横跳”却抓不住重点，希望我的这些记录能陪你一起理清思路。\n这不仅是一份应聘时的展示作品，更是一份属于我自己的技术资产。\n\n下一篇预告：《YOLOv1：将检测视为回归的极简美学》—— 聊聊那个一切开始的地方。\n","categories":["目标检测"],"tags":["YOLO系列","YOLO","目标检测","深度学习"]},{"title":"transformer 手搓过程及思考","url":"/2025/09/22/04-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/transformer-%E6%89%8B%E6%90%93%E8%BF%87%E7%A8%8B%E5%8F%8A%E6%80%9D%E8%80%83/","content":"transformer 手搓过程及思考1. 前言最近在研究transformer，发现网上很多教程都是直接给出代码，没有详细讲解，导致自己看了很久还是一头雾水，因此决定自己动手实现一个transformer，加深对transformer的理解。\n2. transformer 基本结构transformer的基本结构如下：\n\ntransformer由encoder和decoder两部分组成，encoder和decoder都是由多个相同的层堆叠而成，每个层由多头自注意力机制和前馈神经网络组成。\n参考\nAttention Is All You Need\nhttps://github.com/harvardnlp/annotated-transformer\n\n","categories":["深度学习"],"tags":["LLM","Transformer","手搓"]},{"title":"Unigram Language Model (ULM)","url":"/2026/02/20/05-%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/Unigram-Language-Model/","content":"","categories":["大模型LLM"],"tags":["大模型LLM","分词算法"]},{"title":"大模型微调方法调研","url":"/2025/09/21/04-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94/","content":"大模型微调方法调研\n2019年谷歌的研究人员首次在论文《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。\n\n参考链接\nhttps://blog.csdn.net/acelit/article/details/137838266\n\n","categories":["深度学习"],"tags":["LLM","SFT","调研"]},{"title":"分词算法BPE","url":"/2026/02/20/05-%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/Byte-Pair-Encoding%E5%8E%9F%E7%90%86/","content":"Byte Pair Encoding (BPE)\n概念及其原理\n\n\n简介Byte Pair Encoding (BPE) 是 NLP 中最重要的编码方式之一，它的有效性已被 GPT-2、RoBERTa、XLM、FlauBERT 等强大的语言模型所证实。\n\n初识 BPEBPE 是一种简单的数据压缩算法，它在 1994 年发表的文章”A New Algorithm for Data Compression”中被首次提出。\n核心思想\nBPE 每一步都将最常见的一对相邻数据单位替换为该数据中没有出现过的一个新单位，反复迭代直到满足停止条件。\n\n\n压缩示例假设我们有需要编码（压缩）的数据 aaabdaaabac。\n\n相邻字节对 aa 最常出现，用新字节 Z 替换\n\n结果：ZabdZabac，其中 Z = aa\n\n\n下一个常见字节对是 ab，用 Y 替换\n\n结果：ZYdZYac，其中 Z = aa，Y = ab\n\n\n继续递归编码 ZY 为 X\n\n最终结果：XdXac，其中 X = ZY，Y = ab，Z = aa\n\n\n无法进一步压缩，因为没有重复出现的字节对\n\n\n解码：反向执行以上过程即可还原原始数据。\n\nBPE 在 NLP 中的应用核心逻辑：子词分词 (Subword Tokenization)在自然语言处理中，BPE 不再仅仅是为了压缩体积，而是为了在词表大小与语义表示之间寻找完美的平衡。\n词表 (Vocabulary) 是什么？\n词表（Vocabulary）就是模型可以识别和生成的全部 token 的集合。\n\n在大模型中，模型并不能直接理解”字符串”，而是先把文本切分成 token，再将 token 转换为向量进行计算。\n文本处理流程文本 → 分词(tokenize) → token id → embedding向量 → Transformer计算\n\n简单来说：\n\n💡 词表 &#x3D; 所有可能 token 的全集\n\n\n词表在模型中的位置词表本身只是索引表，但它直接影响两个核心部分：\n1️⃣ Embedding 表Embedding 矩阵形状： [vocab_size, hidden_size]\n示例计算：\n\n\n\n参数\n值\n\n\n\nvocab_size\n50,000\n\n\nhidden_size\n4096\n\n\nEmbedding 参数量\n50000 × 4096 ≈ 2亿\n\n\n2️⃣ 输出层（LM Head）模型预测下一个 token 时，需要输出一个长度为 vocab_size 的概率分布。\n输出层权重形状： [hidden_size, vocab_size]\n\n\n\n参数\n值\n\n\n\nhidden_size\n4096\n\n\nvocab_size\n50,000\n\n\n输出层参数量\n4096 × 50000 ≈ 2亿\n\n\n\n⚠️ 结论：词表越大，模型参数越多\n\n\n💡 注意： 词表大小不会改变单个句子的输入矩阵形状，输入矩阵大小只与 seq_len × hidden_size 有关。\n\n\n词表大小的权衡对比情况示例输入句子：我喜欢机器学习\n\n\n\n词表大小\n词表内容\n分词结果\nseq_len\n\n\n\n大词表 (50k)\n包含”喜欢”、”机器学习”\n我 / 喜欢 / 机器学习\n3\n\n\n小词表 (2k)\n无完整词，需拆分\n我 / 喜 / 欢 / 机 / 器 / 学 / 习\n7\n\n\n影响对比\n\n\n维度\n大词表 (50k)\n小词表 (2k)\n\n\n\n参数量\n约 2亿\n约 800万\n\n\n序列长度\n较短 (3)\n较长 (7)\n\n\nAttention 计算量\nO(3²) &#x3D; 9\nO(7²) &#x3D; 49\n\n\n语义表达\n完整、准确\n碎片化\n\n\n\n词表越大 vs 词表越小词表越大\n\n\n✅ 优点\n❌ 缺点\n\n\n\n• 序列更短• attention 计算更少• 语义表达更完整• 推理速度更快（长文本场景）\n• embedding 参数增加• 输出层参数增加• softmax 计算变重• 低频词占用空间\n\n\n词表越小\n\n\n✅ 优点\n❌ 缺点\n\n\n\n• 参数更少，模型更轻量• 泛化能力强（可组合新词）• 高频子词训练更稳定\n• 序列变长• attention 计算量增加• 推理变慢• 语义表达碎片化\n\n\n\n总结词表决定：\n\n模型能表达多少种基本单位\nembedding 表大小\n输出层大小\n句子被切成多少 token\n\n核心权衡：\n\n\n\n词表大小\n参数量\n序列长度\n\n\n\n越大\n更多\n更短\n\n\n越小\n更少\n更长\n\n\n\n💡 而 BPE 的意义就在于：用”子词”在参数规模与计算效率之间找到最优折中。\n\n\n核心术语\n\n\n术语\n说明\n\n\n\nToken (符号&#x2F;令牌)\n模型处理的最小单元。可以是完整单词（如 Only），也可以是一个字母或字符\n\n\nTokenize (分词)\n将连续句子拆解为一个个 Token 的过程\n\n\nSubword (子词)\n介于单词和字符之间的单位。例如 learning 会被拆分为 learn（主语素）和 ##ing（后缀）\n\n\n\nBPE 的”NLP 变体”工作原理与单纯的字符串压缩不同，NLP 中的 BPE 遵循以下原则：\n高频词一体化确保最常见的词（如 the、of、我、你好）在词表中表现为单个 Token。\n低频词子词化罕见的词会被分解为两个或多个子词单元（Subword tokens）。\n\n示例：假设词表里有 happily，但没有 unhappily。BPE 会将其拆解为 un + happily。这样模型即使没见过完整单词，也能通过 un 猜出它是”不快乐”的意思。\n\n\n为什么它是 Transformer 的标配？\n\n\n优势\n说明\n\n\n\n解决 OOV (词汇溢出)\n理论上，只要词表包含所有基础字符，模型就能通过组合子词拼凑出任何生僻词\n\n\n语义对齐\n子词往往携带语义（如词根、前缀），有助于模型理解词义\n\n\n效率最优\n相比于纯字符序列（太长）或纯单词序列（词表太大），子词序列长度适中\n\n\n\n两个版本对比\n\n\n维度\n1994 压缩算法版\nNLP 变体版 (Transformer)\n\n\n\n处理对象\n任意字节流 (Bytes)\n语言序列 (Text&#x2F;UTF-8)\n\n\n最终目标\n最小化存储空间\n优化模型理解与生成的概率\n\n\n停止条件\n无法进一步压缩\n达到预设词表大小 (Vocab Size)\n\n\n典型单位\n字节对 (Byte Pair)\n子词单元 (Subword Unit)\n\n\n\nBPE 算法实战演示\n“光听不练永远无法掌握精髓”，让我们通过一个实际的 NLP 案例来拆解 BPE 的核心逻辑。\n\n\n1. 初始化：预分词与频次统计假设我们的语料库在经过初步分词（Pre-tokenization）后，得到以下四个单词及其出现频率：\n&#123;&quot;old&quot;: 7, &quot;older&quot;: 3, &quot;finest&quot;: 9, &quot;lowest&quot;: 4&#125;\n\n\n2. 标记边界：引入结束符 &lt;/w&gt;为了让算法能够精准识别单词边界，我们在每个单词末尾添加特殊的结束标记 &lt;/w&gt;。\n&#123;&quot;old&lt;/w&gt;&quot;: 7, &quot;older&lt;/w&gt;&quot;: 3, &quot;finest&lt;/w&gt;&quot;: 9, &quot;lowest&lt;/w&gt;&quot;: 4&#125;\n\n\n为什么要加 &lt;/w&gt;？\n\n防止跨词合并：在统计相邻字符对（Pair）时，如果不加标记，算法可能会错误地将 A 单词的词尾和 B 单词的词头当成一对\n学习位置特征：&lt;/w&gt; 会被视为字符对的一部分，帮助算法学习哪些子词（如 est）更倾向于出现在词尾\n\n\n\n3. 拆解：建立初始基础词表我们将单词进一步拆分为最小的字符单位。此时的”初始 Token 集合”由语料中所有出现的独立字符及 &lt;/w&gt; 组成。\n初始词表及频率统计表\n\n\n序号\nToken\n出现频次\n来源逻辑举例\n\n\n\n1\n&lt;/w&gt;\n23\n7(old) + 3(older) + 9(finest) + 4(lowest)\n\n\n2\no\n14\n7(old) + 3(older) + 4(lowest)\n\n\n3\nl\n14\n7(old) + 3(older) + 4(lowest)\n\n\n4\nd\n10\n7(old) + 3(older)\n\n\n5\ne\n16\n3(older) + 9(finest) + 4(lowest)\n\n\n6\nr\n3\n3(older)\n\n\n7\nf\n9\n9(finest)\n\n\n8\ni\n9\n9(finest)\n\n\n9\nn\n9\n9(finest)\n\n\n10\ns\n13\n9(finest) + 4(lowest)\n\n\n11\nt\n13\n9(finest) + 4(lowest)\n\n\n12\nw\n4\n4(lowest)\n\n\n\n此时的状态：词表大小（Vocab Size）为 12。下一步，BPE 将开始扫描这些 Token，寻找出现频率最高的相邻”对子”进行合并，并一次又一次地执行相同的迭代，直到达到预设的 token 数限制或迭代限制。\n\n\n迭代机制接下来的每一轮循环中，BPE 算法都会执行以下三个标准化步骤：\n┌─────────────────────────────────────────────────────────┐│                    BPE 迭代流程                          │├─────────────────────────────────────────────────────────┤│  1. 统计 (Count)                                        ││     扫描当前语料库，统计所有相邻 Token 对出现的频率       ││                                                          ││  2. 合并 (Merge)                                        ││     寻找出现频次最高的一组对子，合并为新的 Token          ││     例如：e + s → es                                     ││                                                          ││  3. 更新 (Update)                                       ││     • 将新 Token 加入词表索引                            ││     • 将语料库中对应的字符对替换为新 Token               │└─────────────────────────────────────────────────────────┘\n\n\n停止条件 (Termination Criteria)算法会反复执行上述迭代过程，直到触发以下任一条件：\n\n\n\n条件\n说明\n\n\n\n达到预设词表上限\n例如设置 vocab_size = 30000，当合并出的新词加上原始字符达到这个数字时停止\n\n\n达到迭代次数限制\n手动指定合并操作执行 N 次\n\n\n收益递减\n当最高频对子的出现次数低于某个阈值（如 1 次），代表无法进一步压缩语义\n\n\n\n最终结果：原本零散的字符（如 o、l、d）会逐渐演变成完整的子词（如 old）。这种方式让模型既能认识常见的完整词汇，也能通过拼凑子词来理解从未见过的生僻词。\n\n\n完整迭代过程\n迭代 1最常见的字节对是 e 和 s（在 finest 和 lowest 中），出现 9 + 4 &#x3D; 13 次。合并为新的 token es。\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n23\n\n\n2\no\n14\n\n\n3\nl\n14\n\n\n4\nd\n10\n\n\n5\ne\n16 − 13 &#x3D; 3\n\n\n6\nr\n3\n\n\n7\nf\n9\n\n\n8\ni\n9\n\n\n9\nn\n9\n\n\n10\ns\n13 − 13 &#x3D; 0\n\n\n11\nt\n13\n\n\n12\nw\n4\n\n\n13\nes\n13\n\n\n\n迭代 2合并 token es 和 t，出现 13 次。形成新 token est。\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n23\n\n\n2\no\n14\n\n\n3\nl\n14\n\n\n4\nd\n10\n\n\n5\ne\n3\n\n\n6\nr\n3\n\n\n7\nf\n9\n\n\n8\ni\n9\n\n\n9\nn\n9\n\n\n10\ns\n0\n\n\n11\nt\n13 − 13 &#x3D; 0\n\n\n12\nw\n4\n\n\n13\nes\n13 − 13 &#x3D; 0\n\n\n14\nest\n13\n\n\n\n迭代 3字节对 est 和 &lt;/w&gt; 出现 13 次。合并为 est&lt;/w&gt;。\n\n注意：合并停止 token &lt;/w&gt; 非常重要。这有助于算法理解 “estimate” 和 “highest” 等词之间的区别。这两个词都有一个共同的 “est”，但一个词在结尾有 “est” token，一个在开头。\n\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n23 − 13 &#x3D; 10\n\n\n2\no\n14\n\n\n3\nl\n14\n\n\n4\nd\n10\n\n\n5\ne\n3\n\n\n6\nr\n3\n\n\n7\nf\n9\n\n\n8\ni\n9\n\n\n9\nn\n9\n\n\n10\ns\n0\n\n\n11\nt\n0\n\n\n12\nw\n4\n\n\n13\nes\n0\n\n\n14\nest\n13 − 13 &#x3D; 0\n\n\n15\nest&lt;/w&gt;\n13\n\n\n\n迭代 4字节对 o 和 l 出现 7 + 3 &#x3D; 10 次。合并为 ol。\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n10\n\n\n2\no\n14 − 10 &#x3D; 4\n\n\n3\nl\n14 − 10 &#x3D; 4\n\n\n4\nd\n10\n\n\n5\ne\n3\n\n\n6\nr\n3\n\n\n7\nf\n9\n\n\n8\ni\n9\n\n\n9\nn\n9\n\n\n10\ns\n0\n\n\n11\nt\n0\n\n\n12\nw\n4\n\n\n13\nes\n0\n\n\n14\nest\n13\n\n\n15\nol\n10\n\n\n\n迭代 5字节对 ol 和 d 出现 10 次。合并为 old。\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n10\n\n\n2\no\n4\n\n\n3\nl\n4\n\n\n4\nd\n10 − 10 &#x3D; 0\n\n\n5\ne\n3\n\n\n6\nr\n3\n\n\n7\nf\n9\n\n\n8\ni\n9\n\n\n9\nn\n9\n\n\n10\ns\n0\n\n\n11\nt\n0\n\n\n12\nw\n4\n\n\n13\nes\n0\n\n\n14\nest\n0\n\n\n15\nest&lt;/w&gt;\n13\n\n\n16\nol\n10 − 10 &#x3D; 0\n\n\n17\nold\n10\n\n\n\n最终 Token 列表现在 f、i 和 n 的频率为 9，但只有一个单词包含这些字符，因此不再合并。\n\n\n\nNumber\nToken\nFrequency\n\n\n\n1\n&lt;/w&gt;\n10\n\n\n2\no\n4\n\n\n3\nl\n4\n\n\n4\ne\n3\n\n\n5\nr\n3\n\n\n6\nf\n9\n\n\n7\ni\n9\n\n\n8\nn\n9\n\n\n9\nw\n4\n\n\n10\nest&lt;/w&gt;\n13\n\n\n11\nold\n10\n\n\n\nToken 列表从 12 减少到 11，说明 token 列表被有效压缩了。\n\n\n为什么频率要相减？┌──────────────────────────────────────────────────────────┐│                                                          ││   Token 的频率代表了它在语料库中&quot;独立存在&quot;的次数。        ││                                                          ││   当你把 e 和 s 合并为 es 时，                            ││   这部分 e 和 s 就已经不再是&quot;独立的个体&quot;了，               ││   它们变成了 es 的组成部分。                              ││                                                          ││   如果不减去，系统会认为语料库中既有 13 个新 Token es，    ││   又有原先的 13 个 e 和 s，                               ││   这会导致统计数据双重计算（Double Counting）。           ││                                                          │└──────────────────────────────────────────────────────────┘\n\n\n算法特点在实际的大规模语料库中，BPE 能够通过更多迭代次数将 token 列表缩小更多的比例。\n\n注意：token 计数既能增加也能减少，也能保持不变。在实际应用中，token 计数通常先增加然后减少。我们选择一个最合适的停止标准，以便数据集可以以最有效的方式分解为 token。\n\n\n编码和解码解码 (Decoding)解码很简单：将所有 token 连接在一起获得整个单词。\n\n示例：编码序列 [&quot;the&lt;/w&gt;&quot;, &quot;high&quot;, &quot;est&lt;/w&gt;&quot;, &quot;range&lt;/w&gt;&quot;, &quot;in&lt;/w&gt;&quot;, &quot;Seattle&lt;/w&gt;&quot;]\n解码为：[&quot;the&quot;, &quot;highest&quot;, &quot;range&quot;, &quot;in&quot;, &quot;Seattle&quot;]\n而不是：[&quot;the&quot;, &quot;high&quot;, &quot;estrange&quot;, &quot;in&quot;, &quot;Seattle&quot;]\n因为 est 中存在 &lt;/w&gt; token。\n\n\n编码 (Encoding)编码计算复杂度比较高。算法步骤如下：\n\n遍历语料库中的所有 token —— 从最长到最短\n尝试用这些 token 替换给定单词序列中的子字符串\n最终所有子字符串将被替换为 token 列表中已存在的 token 组合\n如果有剩余未知子串，用 unknown token 替换\n\n\n在实践中，我们将 tokenized 好的单词保存在字典中。对于未知（新）词，我们应用上述编码方法进行 tokenization，并将新词的 token 添加到字典中以备将来使用。\n\n\n算法性质BPE 是贪心算法吗？\n是的。为了以最有效的方式构建语料库，BPE 在迭代时通过比较 token 的频率大小来穷尽每一种可能，遵循一种贪婪的策略来尽可能取得最优的解决方案。\n\n尽管贪婪，但 BPE 具有良好的性能，并已成为机器翻译等主流 NLP 任务的首选 tokenize 方法之一。\n\n参考链接\nhttps://zhuanlan.zhihu.com/p/424631681\nhttps://leimao.github.io/blog/Byte-Pair-Encoding/\n\n\n\n\nBPE 的动态本质\n它不是静态的切分，而是一个根据统计结果不断进化的压缩过程。\n\n","categories":["大模型LLM"],"tags":["大模型LLM","分词算法"]},{"title":"WordPiece 分词算法原理","url":"/2026/02/20/05-%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/WordPiece%20%E5%8E%9F%E7%90%86/","content":"WordPiece 分词算法\nGoogle BERT 的核心分词技术\n\n\n简介既然你已经对 BPE 的”频次合并”逻辑很熟悉了，那么理解 WordPiece 就会有一种”更进一步”的感觉。\n如果说 BPE 是单纯的统计学家，那么 WordPiece 就像是一个概率学家。\n它是 Google 在 2016 年为了解决神经机器翻译问题提出的，后来因为成为了 BERT 的默认分词方案而彻底走红。自此之后，很多基于 BERT 的 Transformer 模型都复用了这种方法，比如 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET。\n\n核心思想：似然增量WordPiece 和 BPE 的最大区别在于合并的标准。\n\n\n\n算法\n核心逻辑\n\n\n\nBPE\n看数量。哪两个字符凑在一起的次数最多，我就合并谁。\n\n\nWordPiece\n看概率。合并这两个字符，能不能让整个语料库出现的概率（似然值）提升得最多？\n\n\n💡 核心公式在选择合并 c₁ 和 c₂ 生成 c₁c₂ 时，WordPiece 衡量的是：\nscore(c₁, c₂) = log P(c₁c₂) - [log P(c₁) + log P(c₂)]\n\n(注：在实际操作中，这等同于点互信息 PMI)\n公式背后的逻辑如果 c₁ 和 c₂ 经常在一起出现（分子大），且它们很少单独出现（分母小），分数就会很高。\n直观例子假设语料库里有这些词：\n\n\n\n词汇\n关系\n\n\n\n“深度” + “学习”\n经常成对出现\n\n\n“的” + “学”\n虽然单独出现很多，但不是有意义的组合\n\n\n虽然 “的” 这个字出现次数极多，但 “的” 和 “学” 组合在一起的概率并不比它们随机碰上的概率高多少。而 “深度” 和 “学习” 一旦出现，往往是成对的，所以算法会优先合并 “深度学习”。\n\nWordPiece vs BPE：核心区别全面对比表\n\n\n维度\nBPE (字节对编码)\nWordPiece (词片)\n\n\n\n代表模型\nGPT 系列, Llama, RoBERTa\nBERT, DistilBERT, Electra\n\n\n合并逻辑\n最高频 (Count)\n最高概率提升 (Likelihood)\n\n\n选择标准\n频率最高\n最大化似然增量\n\n\n子词标记\n通常在词首加 Ġ (GPT)\n非词首加 ##\n\n\n分词算法\n贪心匹配\n概率最大化\n\n\n算法本质\n纯粹的数据压缩\n统计语言建模\n\n\n计算复杂度\n低\n中\n\n\n\n💡 为什么 BERT 选择了 WordPiece？WordPiece 这种”互信息”式的合并方式，能够更有效地把有意义的词根和前缀提取出来。\n示例： 对于 unhappy，WordPiece 能更敏锐地察觉到 un 和 happy 之间的独立性，而不仅仅是因为它们凑巧在一起出现得多。\n\n对比视角：\n\nBPE：可能只因为 “un” 后面经常跟 “h” 就合并\nWordPiece：会判断 “un” 作为一个否定前缀的独立性，更合理地分词为 un + ##happy\n\n\n\nWordPiece 算法实战演示\n“光听不练永远无法掌握精髓”，让我们通过一个实际的案例来拆解 WordPiece 的核心逻辑。\n\n\n第一步：初始化假设我们有一组单词及其频次：\n&#123;&quot;hug&quot;: 10, &quot;pug&quot;: 5, &quot;pun&quot;: 12, &quot;bun&quot;: 4&#125;\n\n基础词表h, u, g, p, n, b\n\n添加 ## 标记WordPiece 有个特殊规则：非词首的子词要加 ##。\n拆解结果：\n\n\n\n原词\n拆解后\n\n\n\nhug\nh, ##u, ##g\n\n\npug\np, ##u, ##g\n\n\npun\np, ##u, ##n\n\n\nbun\nb, ##u, ##n\n\n\n\n💡 ## 的作用： 标记子词是否为单词的延续部分\n\n无 ## 前缀：单词开头\n有 ## 前缀：单词中间或结尾\n\n\n\n第二步：计算 Score 并合并我们要决定合并哪一对。让我们比较两个候选对。\n候选对 1：##u + ##g统计数据：\n\n\n\n数据项\n值\n\n\n\nCount(##u, ##g)\n10 (hug) + 5 (pug) &#x3D; 15\n\n\nCount(##u)\n10 + 5 + 12 + 4 &#x3D; 31\n\n\nCount(##g)\n10 + 5 &#x3D; 15\n\n\n候选对 2：##u + ##n统计数据：\n\n\n\n数据项\n值\n\n\n\nCount(##u, ##n)\n12 (pun) + 4 (bun) &#x3D; 16\n\n\nCount(##n)\n12 + 4 &#x3D; 16\n\n\n计算得分通过不断比较这些得分，WordPiece 会选择得分最高的一对进行合并。\n\n迭代过程WordPiece 会持续迭代，每一轮都：\n┌─────────────────────────────────────────────────────────┐│                  WordPiece 迭代流程                      │├─────────────────────────────────────────────────────────┤│  1. 统计 (Count)                                        ││     统计所有相邻 token 对的出现频率                      ││                                                          ││  2. 计算得分 (Score)                                    ││     score = log P(pair) - log P(c₁) - log P(c₂)         ││     选择得分最高的对子                                   ││                                                          ││  3. 合并 (Merge)                                        ││     将选定的对子合并为新的 token                         ││     例如：##u + ##g → ##ug                              ││                                                          ││  4. 更新 (Update)                                       ││     • 将新 token 加入词表                                ││     • 更新语料库中的 token 表示                          │└─────────────────────────────────────────────────────────┘\n\n\n停止条件迭代持续进行，直到满足以下任一条件：\n\n\n\n条件\n说明\n\n\n\n达到词表大小限制\n例如 vocab_size = 30,000\n\n\n得分低于阈值\n当最高得分低于某个阈值时（如 1e-5）\n\n\n达到迭代次数\n手动指定合并操作执行 N 次\n\n\n\n分词算法：Longest Word First与编码过程不同，WordPiece 在对新句子进行分词时，采用的是从左到右的最长匹配策略。\n算法步骤输入：单词 &quot;unaffable&quot;1. 先看最长能匹配到什么？匹配到 `un`2. 剩下的部分是 `affable`3. 在词表里找以 `##` 开头的最长匹配，匹配到 `##affable`4. 最终结果：[&quot;un&quot;, &quot;##affable&quot;]\n\n举例说明假设词表里有 [&quot;un&quot;, &quot;##affable&quot;, &quot;##able&quot;]\n输入：unaffable\n\n\n步骤\n匹配\n剩余\n说明\n\n\n\n1\nun\naffable\n最长前缀匹配\n\n\n2\n##affable\n&quot;&quot;\n最长 ## 前缀匹配\n\n\n结果\n[&quot;un&quot;, &quot;##affable&quot;]\n\n✅ 成功\n\n\n处理未知词如果中间有一段在词表中找不到，整个词就会被标记为 [UNK]。\n\n⚠️ 注意： 与 BPE 不同，WordPiece 使用的是最长匹配而非概率最大化的分词策略（这是 BERT 的实际实现方式）。\n\n\nWordPiece 与 BPE 分词对比相同输入，不同输出输入单词：unhappilyBPE 分词（贪心匹配）词表：[u, un, unh, h, ha, hap, happ, happi, happily, ##ly, ##y]过程：1. 最长匹配前缀：unh2. 剩余：appily3. 最长匹配前缀：happ4. 剩余：ily5. 匹配：##ily结果：unh happ ##ily\n\nWordPiece 分词（最长匹配）词表：[un, unh, happily, happy, ##h, ##app, ##ily, ##ly]过程：1. 最长匹配前缀：un2. 剩余：happily3. 匹配：##happily结果：un ##happily\n\n关键区别\n\n\n算法\n策略\n特点\n\n\n\nBPE\n局部贪心\n每次选择最长的可能匹配，不考虑整体概率\n\n\nWordPiece\n训练时用概率，分词时用最长匹配\n训练时考虑统计依赖，分词时追求效率\n\n\n\nWordPiece 的数学本质信息论视角合并得分公式可以重写为：\nscore(c₁, c₂) = log [P(c₁c₂) / (P(c₁) × P(c₂))]\n\n这正是 点互信息（Pointwise Mutual Information, PMI）：\n\nPMI 越高，说明 c₁ 和 c₂ 越倾向于一起出现，而不是偶然相遇。\n\nPMI 的直观理解PMI(x, y) = log [联合概率 / (边际概率之积)]高 PMI → x 和 y 有强烈的关联低 PMI → x 和 y 几乎独立负 PMI → x 和 y 互斥\n\n\nWordPiece 的优缺点优点\n\n\n✅ 优势\n说明\n\n\n\n语义感知\n基于 PMI 的合并能保留有意义的词根和前缀\n\n\n稳定性好\n在 BERT 等模型上表现优异\n\n\n处理 OOV\n通过子词组合解决未知词问题\n\n\n减少无效合并\n不会因为高频但无意义的字符对而合并\n\n\n缺点\n\n\n❌ 劣势\n说明\n\n\n\n计算复杂\n每次合并需要计算所有候选对的得分\n\n\n训练成本高\n需要多轮迭代统计\n\n\n依赖预训练\n需要大规模语料库训练词表\n\n\n不透明\n相比 BPE 的直观频率统计，概率模型更难理解\n\n\n\n实际应用：BERT 分词示例BERT Base 词表配置\n\n\n参数\n值\n\n\n\n词表大小\n30,000\n\n\n特殊标记\n[PAD], [UNK], [CLS], [SEP], [MASK]\n\n\n最大序列长度\n512\n\n\n分词示例输入文本text = &quot;WordPiece is a tokenization algorithm&quot;\n\nBERT 分词结果[&quot;Word&quot;, &quot;##Piece&quot;, &quot;is&quot;, &quot;a&quot;, &quot;token&quot;, &quot;##ization&quot;, &quot;algorithm&quot;]\n\nToken ID 序列[10234, 15968, 2003, 1037, 19204, 10631, 14213, 17953]\n\n注意事项\n⚠️ WordPiece 倾向于：\n\n将罕见词拆分为更小的子词\n保留常见完整词\n使用 ## 标记子词延续\n\n\n\n三种分词算法全对比\n\n\n特性\nBPE\nWordPiece\nUnigram\n\n\n\n训练方向\n自底向上（字符→词）\n自底向上\n自顶向下（词→字符）\n\n\n选择标准\n频率\n似然概率 (PMI)\n损失降低\n\n\n分词策略\n贪心匹配\n最长匹配\n最短路径 (Viterbi)\n\n\n典型应用\nGPT 系列, Llama\nBERT 系列\nT5, ALBERT\n\n\n计算复杂度\n低\n中\n高\n\n\n\n实战：从零实现 WordPiece训练算法伪代码from collections import Counterdef train_wordpiece(corpus, vocab_size, threshold=1e-5):    # 初始化：将所有词拆分为字符    vocab = set(char for word in corpus for char in word)    vocab.add(&#x27;##&#x27;)    # 初始频率统计    word_freqs = Counter(corpus)    while len(vocab) &lt; vocab_size:        # 计算所有可能合并的得分        best_pair = None        best_score = -float(&#x27;inf&#x27;)        for pair in get_all_pairs(corpus):            score = compute_score(pair, word_freqs)            if score &gt; best_score:                best_score = score                best_pair = pair        # 停止条件        if best_score &lt; threshold:            break        # 合并最优对        merge_pair(best_pair, corpus)        vocab.add(best_pair[0] + best_pair[1])    return vocabdef compute_score(pair, word_freqs):    &quot;&quot;&quot;计算合并得分：log P(ab) - log P(a) - log P(b)&quot;&quot;&quot;    total = sum(word_freqs.values())    pair_count = count_pair(pair, word_freqs)    c1_count = word_freqs.get(pair[0], 0)    c2_count = word_freqs.get(pair[1], 0)    if pair_count == 0 or c1_count == 0 or c2_count == 0:        return -float(&#x27;inf&#x27;)    score = (math.log(pair_count / total) -             math.log(c1_count / total) -             math.log(c2_count / total))    return score\n\n分词算法伪代码def tokenize_word(word, vocab):    &quot;&quot;&quot;WordPiece 分词：最长匹配策略&quot;&quot;&quot;    if word in vocab:        return [word]    tokens = []    remaining = word    while remaining:        # 尝试最长的可能匹配        longest_match = None        for length in range(len(remaining), 0, -1):            prefix = remaining[:length]            candidate = prefix if not tokens else &#x27;##&#x27; + prefix            if candidate in vocab:                longest_match = candidate                remaining = remaining[length:]                break        if longest_match is None:            # 未找到匹配，返回 [UNK]            return [&#x27;[UNK]&#x27;]        tokens.append(longest_match)    return tokens\n\n\n当前趋势主流大模型的选择\n💡 现状： 目前主流的大模型（如 Llama 3、Mistral）多回归到了 BPE（尤其是 Byte-level BPE）。\n\n为什么回归 BPE？\n\n\n原因\n说明\n\n\n\n多语言支持\nByte-level BPE 能处理任何语言的字符\n\n\n鲁棒性\n处理特殊符号、emoji 等更加稳定\n\n\n简洁高效\n实现简单，训练和推理速度快\n\n\n社区生态\nHuggingFace 等工具链对 BPE 支持更好\n\n\nWordPiece 的定位虽然 BPE 更流行，但 WordPiece 仍然是 BERT 系列（及其变体）的核心技术，在理解语义和保持词根完整性方面仍有独特优势。\n\n参考链接\nhttps://arxiv.org/abs/1609.08144 - Japanese and Korean Voice Search (WordPiece 原始论文)\nhttps://arxiv.org/abs/1810.04805 - BERT: Pre-training of Deep Bidirectional Transformers\nhttps://huggingface.co/docs/tokenizers - HuggingFace Tokenizers 文档\n\n\n\n\nWordPiece 的本质\n它不只是分词算法，更是连接统计语言模型与深度神经网络的桥梁。\n用概率的眼光看语言，让模型真正理解”词”的意义。\n","categories":["大模型LLM"],"tags":["大模型LLM","分词算法"]},{"title":"stm32矩阵按键原理及程序","url":"/2025/09/20/06-%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/stm32%E7%9F%A9%E9%98%B5%E6%8C%89%E9%94%AE%E5%8E%9F%E7%90%86%E5%8F%8A%E7%A8%8B%E5%BA%8F/","content":"\n矩阵键盘原理矩阵键盘是一种高效检测多个按键的方法，通过行列交替扫描来节省GPIO口资源。在STM32中，只需合理配置GPIO输入输出模式，就能实现4×4矩阵键盘的检测逻辑。\n初始化设置列初始化：将所有列（PF0, PF1, PF2, PF3）设置为上拉输入模式。\n行初始化：将所有行（PF4, PF5, PF6, PF7）设置为推挽输出模式。\n\n扫描过程选择列：从第一列开始，依次选择每一列进行扫描。\n输出低电平：当选择某一列时，将该列设为低电平，其他列为高电平。\n检测行状态：检测所有行的状态，如果某一行检测到低电平，则表示该行与当前列交叉处的按键被按下。\n\n示例说明假设我们正在扫描第一列（PF0），此时输出低电平，并检测行的状态：\n如果F7检测到低电平（即PF7为低电平），则表示S1按键被按下。\n如果F6检测到低电平（即PF6为低电平），则表示S2按键被按下。\n以此类推，可以检测到第一行的所有按键值（S1, S2, S3, S4）。\n\n同理，通过扫描其他列，可以检测到整个矩阵键盘上的所有按键值（共16个按键）。\n程序下载完整示例程序（含 STM32 矩阵键盘扫描代码）：  \n\n📥 链接：点击下载  \n🔑 提取码：r3hs\n\n总结矩阵键盘通过行列交替扫描，可以高效检测多个按键，节省IO口资源。在STM32中，只需合理配置GPIO输入输出模式，就能实现4×4矩阵键盘的检测逻辑。\n🚀 下一步可以尝试：  \n\n加入按键消抖处理：消除按键按下时产生的抖动现象，提高按键检测的准确性。\n支持多键同时按下：实现对多个按键同时按下的检测功能，扩展键盘的应用场景。\n与中断或RTOS配合使用：结合中断或实时操作系统（RTOS），进一步优化键盘扫描的效率和响应速度。\n\n","categories":["嵌入式开发"]}]